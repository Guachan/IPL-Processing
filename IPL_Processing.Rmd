---
title: "IPL Data LowpHOX-2"
author: "Sebastian Cantarero"
date: "8/13/2018"
output: html_document
---

```{r, eval=FALSE}
install.packages("here")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plyr) # recommend not using plyr, dplyr is faster and better implemented
library(tidyverse)
library(ggplot2) # no need to load, part of tidyverse
library(latex2exp)
library(readxl)
library(purrr) # no need to load, part of tidyverse
library(here) # omg, so useful for paths to stuff in your project
library(broom) # bring the broom! it's great for working with models
```


## Sample Data Import

```{r}
## Testing out different methods for importing data
##import csv files from HPLC computer (Quantitation reports) from working directory
## Can apply whatever filename/dataframe names you want here
OG22_T5_250_0_3_SC_2 <- read_csv("OG22_T5-250-0-3_SC_2.csv")

# oh oh - absolute paths and a lot of copy/paste code, time for a function :)
# take a look at how the data_frame map combination is implemented below, this would make this a lot easier
OG23_T5_60_0_3_SC_2 <- read_csv("~/R/Processing IPLs/OG23_T5-60-0-3_SC_2.csv")
OG24_T5_45_0_3_SC_2 <- read_csv("~/R/Processing IPLs/OG24_T5-45-0-3_SC_2.csv")
OG31_T3_55_0_3_SC_2 <- read_csv("~/R/Processing IPLs/OG31_T3-55-0-3_SC_2.csv")
OG32_T3_25_0_3_SC_2 <- read_csv("~/R/Processing IPLs/OG32_T3-25-0-3_SC_2.csv")
OG33_T3_9_0_3_SC_2 <- read_csv("~/R/Processing IPLs/OG33_T3-9-0-3_SC_2.csv")
OG34_T5_35_0_3_SC_2 <- read_csv("~/R/Processing IPLs/OG34_T5-35-0-3_SC_2.csv")
OG39_T5_28_0_3_SC_2 <- read_csv("OG39_T5-28-0-3_SC_2.csv")
QOG30_T3_250_0_3_SC_2 <- read_csv("QOG30_T3-250-0-3_SC_2.csv")
QOG37_T3_14_0_3_SC_2 <- read_csv("QOG37_T3-14-0-3_SC_2.csv")

## Create dataframe of all peak areas according to station and depth.

t5_250_2 <- data.frame(OG22_T5_250_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T5',
         Depth = 250) %>% 
  filter(!grepl('N/F', Response)) ##Removes mols with no detection
  

t5_60_2 <- data.frame(OG23_T5_60_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T5',
         Depth = 60) %>% 
  filter(!grepl('N/F', Response))

t3_250_2 <- data.frame(QOG30_T3_250_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T3',
         Depth = 250) %>% 
  filter(!grepl('N/F', Response))

t5_45_2 <- data.frame(OG24_T5_45_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T5',
         Depth = 45) %>% 
  filter(!grepl('N/F', Response))

t3_55_2 <- data.frame(OG31_T3_55_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T3',
         Depth = 55) %>% 
  filter(!grepl('N/F', Response))

t3_9_2 <- data.frame(OG33_T3_9_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T3',
         Depth = 9) %>% 
  filter(!grepl('N/F', Response))

t3_14_2 <- data.frame(QOG37_T3_14_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T3',
         Depth = 14) %>% 
  filter(!grepl('N/F', Response))

t3_25_2 <- data.frame(OG32_T3_25_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T3',
         Depth = 25) %>% 
  filter(!grepl('N/F', Response))

t5_35_2 <- data.frame(OG34_T5_35_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T5',
         Depth = 35) %>% 
  filter(!grepl('N/F', Response))

t5_28_2 <- data.frame(OG39_T5_28_0_3_SC_2) %>% 
  select(Compound, Response) %>% 
  mutate(Station = 'T5',
         Depth = 28) %>% 
  filter(!grepl('N/F', Response))

data <- bind_rows(t5_28_2, t5_35_2, t3_25_2, t3_14_2, t3_9_2, t3_55_2, t5_45_2, t3_250_2, t5_60_2, t5_250_2) %>% 
  filter(!grepl('Flag', Response)) %>% 
  arrange(desc(Compound)) %>% na.omit() %>% unique() ##Removes duplicates and na values
  

##write.csv(data,"Uncalib2_2.csv")



# JHR
# Here's the script I was using for importing a bunch of csvs in one go. 

setwd("Data/") #temporarily set the working directory
temp <- list.files(pattern="*.csv") #makes a list of all the file names ending in .csv
myfiles <- lapply(temp, read.csv) #lapply applies read.csv to the whole temp list, stores in a list
myfiles_bound <- data.table::rbindlist(myfiles, fill = TRUE) #bind all data into one table
# automatically returns to original wd

# You can add options when you read the csv too, like lapply(temp, read.csv, skip = 4), to parse the input a little
```


## Calibration Standard Input (The Hard Way)
```{r}
## Apply calibration -- These calibrations may differ from run to run so it's best to use from the day of your run or the nearest date available. Also the format depending on which standard mix you ran. We could make several of these for different standard mixes.

## I found it easiest to replace all the Std Area and Std Amounts in the spreadsheet with Std_Area and Std_Amounts in excel before importing.

ARCHAEOL_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1ARCHAEOL", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'ARCHAEOL'
    )

MGDAG_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1MGDG", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'MGDAG'
    )

Gly_Cer_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet11-GLC-CER", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'Gly-Cer'
    )

DGTS_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1DGTS", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'DGTS'
    )

C16PA_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C16PA", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PA'
    )

C16PDME_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C16PDME", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PDME'
    )

C16PME_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C16PME", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PME'
    )

C16PE_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C16-PE", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PE'
    )

C16PG_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C16-PG", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PG'
    )

SQDAG_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1SQ-DAG", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'SQDAG'
    )

PC_ARCHAEOL_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1PC-ARCHAEOL", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PC-ARCHAEOL'
    )

PE_ARCHAEOL_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1PE-AR", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PE-ARCHAEOL'
    )

C21_PC_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C21-PC", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PC-C21'
    )

C16_PC_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C16-PC", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PC-C16'
    )

DGDAG_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1DGDG", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'DGDAG'
    )

C16_PAF_Import <- read_excel("Compound Calibration Report_IPLS standards.xlsx", 
    sheet = "Sheet1C16PAF", na = "N/A", 
    skip = 33) %>% mutate(
      Compound = 'PAF-C16'
    )

calib_standards <- rbind.fill(ARCHAEOL_Import, MGDAG_Import, Gly_Cer_Import, DGTS_Import, C16PA_Import, C16PDME_Import, C16PME_Import, C16PE_Import, C16PG_Import, SQDAG_Import, PC_ARCHAEOL_Import, PE_ARCHAEOL_Import, C21_PC_Import, C16_PC_Import, DGDAG_Import, C16_PAF_Import) %>%
  filter(!grepl('N/F', Std_Area)) %>% 
  transform(Std_Area = as.numeric(Std_Area)) %>% 
  filter(Std_Area >0) %>%  ## Some standards possibly did not inject properly?
  data.frame()
```


## Calibration Standard Import (Should be easier if it works)
```{r}
##Must be a way to import all sheets and bind them into dataframe but could not get it to work.

names <- c("Std_Amount", "Std_Area")
# sk: relative path to the data folder and file in it
path <- here("Data", "Compound Calibration Report_IPLS standards.xlsx")
calibs <- 
  here("Data", "Compound Calibration Report_IPLS standards.xlsx") %>%
  excel_sheets() %>%
  # sk: you're trying to rename the sheets in the excel spreadsheet?
  set_names(1:2, nm = names) %>% ## Has something to do with this function
  # this doesn't find your sheets - they have different names in the spreadsheet but if you're just looking at the indices, why not select the sheets that way?
  map_df(~ read_excel("Compound Calibration Report_IPLS standards.xlsx", sheet = .x, range = "B35:C40"), .id = "sheet")

```

SK: let me propose this alternative

```{r}
# location of the report
path <- here("Data", "Compound Calibration Report_IPLS standards.xlsx")
stopifnot(file.exists(path))

# pull out calibrations
calibs <- 
  data_frame(
    sheet_index = 1:2,
    # bonus: not necessary for reading the file
    sheet_name = excel_sheets(path)[sheet_index],
    data = map(sheet_index, ~read_excel(
      path, sheet = .x, 
      range = "B34:C40",
      col_types = c("numeric", "numeric")
    ))
  )
# look at the data
calibs # still nested
calibs %>% unnest(data) # unnested
calibs %>% unnest(data) %>% # plotted
  filter(!is.na(Std_Area)) %>% 
  ggplot() + aes(Std_Amount, Std_Area, color = sheet_name) + 
  geom_smooth(method = "lm") + geom_point(size = 4)
```

Non-linear regressions need useful starting values to converge. Here's the logic for how I'm going to use the linear regression to start the nls.

$$
\begin{aligned}
y &= a e^{bx} \\
\text{Taylor expansion:  } y &= a \left( 1 + bx + \frac{b^2 x^2}{2!} + ...\right) \approx a + abx \\
\text{Infer from linear regression: } a &= c \\
\text{Infer from linear regression: }b &= m/c
\end{aligned}
$$

```{r}
# using safely to catch problems with the non-linear fit
safe_nls <- safely(nls)

# calibration fit
calib_fits <- calibs %>% 
  mutate(
    lm_fit = map(
      data, 
      ~lm(Std_Area ~ Std_Amount, data = .x)
    ),
    lm_summary = map(lm_fit, glance),
    lm_coefs = map(lm_fit, tidy),
    a = map_dbl(lm_coefs, ~filter(.x, term == "(Intercept)")$estimate),
    b = a / map_dbl(lm_coefs, ~filter(.x, term == "Std_Amount")$estimate),
    nls_safe_fit = pmap(
      list(data = data, a = a, b = b),
      function(data, a, b) {
        safe_nls(Std_Area ~ a * exp(b * Std_Amount), data = data, start = list(a = a, b = b))
      }
    ),
    nls_fit_error = map_chr(
      nls_safe_fit, 
      ~if(is.null(.x$error)) {NA_character_} else {.x$error$message}),
    nls_fit = map(nls_safe_fit, "result"),
    nls_summary = map(nls_fit, ~if(!is.null(.x)) { glance(.x) } else { NULL }),
    nls_coefs = map(nls_fit, ~if(!is.null(.x)) { tidy(.x) } else { NULL })
  ) %>% select(-nls_safe_fit, -a, -b) 

# looks like exponential is not a good fit here, even with better starting values
calib_fits

# linear fit summary and coefficients
calib_fits %>% unnest(lm_summary) %>% unnest(lm_coefs)
calib_fits %>% unnest(lm_coefs)
```

```{r}
# use predict to plot calibration fit manually
calib_fits %>% 
  mutate(lm_predict = map(lm_fit, ~.x$model %>% mutate(Std_Area = predict(.x)))) %>% 
  ggplot() + 
  aes(Std_Amount, Std_Area, color = sheet_name) + 
  geom_line(data = function(df) unnest(df, lm_predict), mapping = aes(linetype = "lm")) +
  geom_point(data = function(df) unnest(df, data), size = 4)
```



## Plotting calibration curves with linear and exponential fits
```{r}
ggplot(calib_standards, aes(x = Std_Amount, y = Std_Area)) +
  geom_point(size = 3) + 
  geom_smooth(method = "lm",aes (colour = "Linear"), se = TRUE, size = 1) +  
  scale_color_manual(name = "Fits",
                     breaks = c("Linear"),
                     values = c("blue")) +
 stat_smooth(method = "nls", formula = y ~ a*exp(b*x), start = list(a=1,b=1), ## Can't get an exponential function to plot, what values to choose for a and b?
              aes(colour = "Exponential")) +
    scale_color_manual(name = "Fits",
                     breaks = c("Linear","Exponential"),
                     values = c("red","blue")) +
  facet_wrap(~Compound, scales = 'free')


## Just the linear model for now

ggplot(calib_standards, aes(x = Std_Amount, y = Std_Area)) +
  geom_point(size = 3) + 
 geom_smooth(method = "lm", formula = y~x, family = gaussian(link = 'log')) +
    scale_color_manual(name = "Fits",
                     breaks = c("Linear","Exponential"),
                     values = c("red","blue")) +
  facet_wrap(~Compound, scales = 'free')


## How to extract linear regression intercept and coef to apply calibrations?

lin_fit <- function(dat) {
  the_fit <- lm(Std_Area ~ Std_Amount, dat)
  setNames(data.frame(t(coef(the_fit))), c("intercept", "slope")) ## Not sure I understand why this works
}

lin_fit(calib_standards %>% filter(Compound == "ARCHAEOL")) ##Test function works on 1 compound

lin_fits_df <- calib_standards %>% 
  group_by(Compound) %>% 
  do(lin_fit(.))



```



# JHR things to add


## After examining the fits, it'd be good to add a chunk to cull any outliers
```{r}
# Culling script here...
```


## Applying the numbers in calib_fits to the data

We need to assign a standard to each compound. The "dictionary" type in python would be great for this. From some googling looks like a list is a good approximation in R? Seb?

Option 1:
```{r}
std_assignments <- c("Std1", "Std2", "Std1")
names(std_assignments) <- c("Compound_A", "Compound_B", "Compound_C")

std_assignments # Look at the full dictionary
std_assignments[["Compound_B"]] # Get the std for a given compound
c(std_assignments, "Compound_D" = "Std5") # Add a compound + std pair (and reassign to std_assignments to overwrite)

```

Option 2:
```{r}
# Really we'd probably make this a .csv and import it for easy editing
std_assignments_opt2 <- data.frame(
  Compound = c("Compound_A", "Compound_B", "Compound_C"), 
  Standard = c("Std1", "Std2", "Std1")
)

filter(std_assignments_opt2, Compound == "Compound_B")[1,2] # Get the std for a given compound? This might have type issues
```

Once each compound has a standard assigned to it, we can pull out the fits from the calibration curves.

Should be able to mutate the data frame with all the raw IPL data and add a column with the calibrated area. Theoretically, you could do this by using the name of the compound to call the name of the standard (via the std_assignments table), which could then be used to call the slope and intercept out of the calib_fits results table and applying a linear equation. Pulling out the slope and intercept is lengthy though (see below) - maybe it'd be best to define a function and map it?

```{r}
# One way to select the estimate for the intercept of the Archaeol calibration:
calib_fits %>% unnest(lm_coefs) %>% filter(
  sheet_name == "Sheet1ARCHAEOL",
  term == "(Intercept)"
) %>% select(estimate)

# Another way. Shorter, but requires assigning a new variable
coefs <- filter(calib_fits, sheet_name == "Sheet1ARCHAEOL")["lm_coefs"] %>% unnest(lm_coefs)
coefs[1,2]
```


